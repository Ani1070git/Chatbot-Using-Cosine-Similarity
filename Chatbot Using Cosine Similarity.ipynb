{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da491ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"test.csv\", encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edaa5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = df['Questions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544ee106",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_list = df['Answers'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e2812cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0e887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Removing non-alphanumeric characters\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b226db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_stopwords(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Removing non-alphanumeric characters\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328450f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125e6c8",
   "metadata": {},
   "source": [
    "# Example - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd23fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text: who is sunil gavaskar\n",
      "similarities: [[0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "max_similarity: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can't answer this question.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
    "\n",
    "def get_response(text):\n",
    "    processed_text = preprocess_with_stopwords(text)\n",
    "    print(\"processed_text:\", processed_text)\n",
    "    vectorized_text = vectorizer.transform([processed_text])\n",
    "    similarities = cosine_similarity(vectorized_text, X)\n",
    "    print(\"similarities:\", similarities)\n",
    "    max_similarity = np.max(similarities)\n",
    "    print(\"max_similarity:\", max_similarity)\n",
    "    if max_similarity > 0.6:\n",
    "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
    "        print(\"high_similarity_questions:\", high_similarity_questions)\n",
    "\n",
    "        target_answers = []\n",
    "        for q in high_similarity_questions:\n",
    "            q_index = questions_list.index(q)\n",
    "            target_answers.append(answers_list[q_index])\n",
    "        print(target_answers)\n",
    "\n",
    "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
    "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
    "        print(\"processed_text_with_stopwords:\", processed_text_with_stopwords)\n",
    "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
    "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
    "        closest = np.argmax(final_similarities)\n",
    "        return target_answers[closest]\n",
    "    else:\n",
    "        return \"I can't answer this question.\"\n",
    "\n",
    "get_response('Who is sunil Gavaskar?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d4219",
   "metadata": {},
   "source": [
    "# Example - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b8e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text: what is machin learn\n",
      "similarities: [[0.         0.         0.         0.         0.77627227 0.\n",
      "  0.         0.        ]]\n",
      "max_similarity: 0.7762722680124386\n",
      "high_similarity_questions: ['What is the role of machine learning in data analytics?']\n",
      "['Machine learning plays a crucial role in data analytics by enabling the development of algorithms that can automatically learn from data and make predictions or take actions without being explicitly programmed. It is used for tasks such as classification, regression, clustering, and anomaly detection.']\n",
      "processed_text_with_stopwords: what is machin learn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Machine learning plays a crucial role in data analytics by enabling the development of algorithms that can automatically learn from data and make predictions or take actions without being explicitly programmed. It is used for tasks such as classification, regression, clustering, and anomaly detection.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
    "\n",
    "def get_response(text):\n",
    "    processed_text = preprocess_with_stopwords(text)\n",
    "    print(\"processed_text:\", processed_text)\n",
    "    vectorized_text = vectorizer.transform([processed_text])\n",
    "    similarities = cosine_similarity(vectorized_text, X)\n",
    "    print(\"similarities:\", similarities)\n",
    "    max_similarity = np.max(similarities)\n",
    "    print(\"max_similarity:\", max_similarity)\n",
    "    if max_similarity > 0.6:\n",
    "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
    "        print(\"high_similarity_questions:\", high_similarity_questions)\n",
    "\n",
    "        target_answers = []\n",
    "        for q in high_similarity_questions:\n",
    "            q_index = questions_list.index(q)\n",
    "            target_answers.append(answers_list[q_index])\n",
    "        print(target_answers)\n",
    "\n",
    "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
    "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
    "        print(\"processed_text_with_stopwords:\", processed_text_with_stopwords)\n",
    "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
    "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
    "        closest = np.argmax(final_similarities)\n",
    "        return target_answers[closest]\n",
    "    else:\n",
    "        return \"I can't answer this question.\"\n",
    "\n",
    "get_response('what is machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f17d3c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: language-tool-python in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: pip in c:\\users\\dell\\anaconda3\\lib\\site-packages (from language-tool-python) (23.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (from language-tool-python) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from language-tool-python) (4.65.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\dell\\anaconda3\\lib\\site-packages (from language-tool-python) (0.38.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->language-tool-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->language-tool-python) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->language-tool-python) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests->language-tool-python) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm->language-tool-python) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install language-tool-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16301395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Data Anlytics\n"
     ]
    }
   ],
   "source": [
    "import language_tool_python\n",
    "\n",
    "tool = language_tool_python.LanguageTool('en')  # English language tool\n",
    "\n",
    "text = 'What is Data Anlytics'\n",
    "matches = tool.check(text)\n",
    "\n",
    "# Apply corrections\n",
    "corrected_text = language_tool_python.utils.correct(text, matches)\n",
    "print(corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1098d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
